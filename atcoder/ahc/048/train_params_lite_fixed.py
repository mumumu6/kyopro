#!/usr/bin/env python3
"""
AtCoder AHC048 - è»½é‡ç‰ˆå­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ï¼ˆpickleå•é¡Œä¿®æ­£ç‰ˆï¼‰
30å€‹ç¨‹åº¦ã®ãƒ‡ãƒ¼ã‚¿ã§å‹•ä½œãƒ†ã‚¹ãƒˆã‚’è¡Œã†

Usage:
    python train_params_lite_fixed.py
    â†’ lgbm_params.json ãŒç”Ÿæˆã•ã‚Œã‚‹
"""

import json, subprocess, random, pathlib, tempfile, shutil, multiprocessing as mp
import numpy as np
import lightgbm as lgb
from tqdm import tqdm
import math
import time
import os
import sys
import glob

# è»½é‡è¨­å®š
SOLVER_BIN = "./solver_eval"
MODEL_JSON = "lgbm_params.json"
INPUT_DIR = "./input"
MAX_WORKERS = min(4, mp.cpu_count())

# è»½é‡ç‰ˆè¨­å®š
LITE_SAMPLE_SIZE = 30
QUICK_EVALUATION = True

print(f"ğŸ§ª è»½é‡ç‰ˆå­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ï¼ˆä¿®æ­£ç‰ˆï¼‰")
print(f"   ä¸¦åˆ—å‡¦ç†: {MAX_WORKERS}ã‚³ã‚¢")

def extract_feature(inp: str) -> np.ndarray:
    """åŸºæœ¬çš„ãªç‰¹å¾´é‡ã‚’æŠ½å‡º"""
    lines = inp.strip().split('\n')
    N, K, H, T, D = map(int, lines[0].split())
    
    # æ‰€æœ‰è‰²ã®çµ±è¨ˆé‡
    own_colors = []
    for i in range(1, K + 1):
        r, g, b = map(float, lines[i].split())
        own_colors.append((r, g, b))
    
    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè‰²ã®çµ±è¨ˆé‡  
    target_colors = []
    for i in range(K + 1, K + 1 + H):
        r, g, b = map(float, lines[i].split())
        target_colors.append((r, g, b))
    
    # åŸºæœ¬ç‰¹å¾´é‡ã®ã¿ï¼ˆ17æ¬¡å…ƒï¼‰
    features = [K, H, T, D]
    
    # æ‰€æœ‰è‰²ã®åˆ†æ•£ãƒ»ç¯„å›²
    own_arr = np.array(own_colors)
    features.extend([
        np.var(own_arr[:, 0]),   # Råˆ†æ•£
        np.var(own_arr[:, 1]),   # Gåˆ†æ•£ 
        np.var(own_arr[:, 2]),   # Båˆ†æ•£
        np.ptp(own_arr[:, 0]),   # Rç¯„å›²
        np.ptp(own_arr[:, 1]),   # Gç¯„å›²
        np.ptp(own_arr[:, 2]),   # Bç¯„å›²
    ])
    
    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè‰²ã®çµ±è¨ˆé‡
    target_arr = np.array(target_colors)
    features.extend([
        np.var(target_arr[:, 0]),   # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆRåˆ†æ•£
        np.var(target_arr[:, 1]),   # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆGåˆ†æ•£
        np.var(target_arr[:, 2]),   # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆBåˆ†æ•£
    ])
    
    # è‰²ç©ºé–“ã‚«ãƒãƒ¼ç‡
    min_dists = []
    for tr, tg, tb in target_colors:
        min_dist = min(((tr-or_)**2 + (tg-og)**2 + (tb-ob)**2)**0.5 
                      for or_, og, ob in own_colors)
        min_dists.append(min_dist)
    features.append(np.mean(min_dists))  # å¹³å‡æœ€å°è·é›¢
    features.append(np.std(min_dists))   # è·é›¢ã®æ¨™æº–åå·®
    
    # ã‚³ã‚¹ãƒˆåŠ¹ç‡æŒ‡æ¨™
    features.append(T / H)               # ã‚¿ãƒ¼ãƒ³/ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæ¯”
    features.append(D / T)               # ã‚³ã‚¹ãƒˆå¯†åº¦
    
    return np.array(features, dtype=np.float32)

def evaluate_with_timeout(inp: str, L: int, B: int, P: int, timeout: int = 15) -> float:
    """è»½é‡ç‰ˆè©•ä¾¡é–¢æ•°"""
    try:
        with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:
            f.write(inp)
            input_file = f.name
        
        cmd = [SOLVER_BIN, str(L), str(B), str(P)]
        
        with open(input_file, 'r') as f:
            result = subprocess.run(
                cmd, stdin=f, stdout=subprocess.PIPE, stderr=subprocess.PIPE,
                timeout=timeout, text=True
            )
        
        if result.returncode == 0 and result.stderr:
            try:
                score = float(result.stderr.strip().split()[-1])
                return score
            except:
                pass
        
        return 1e9
        
    except subprocess.TimeoutExpired:
        return 1e9
    except Exception as e:
        print(f"âš ï¸ è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
        return 1e9
    finally:
        if 'input_file' in locals():
            try:
                os.unlink(input_file)
            except:
                pass

def find_best_params_lite(inp: str) -> tuple[int, int, int, float]:
    """è»½é‡ç‰ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢"""
    best_score = 1e9
    best_params = (2, 8, 0)
    
    # ç²—ã„æ¢ç´¢ã®ã¿ï¼ˆé«˜é€ŸåŒ–ï¼‰
    candidates = [
        (1, 4, 0), (2, 8, 0), (2, 12, 1), 
        (3, 16, 2), (4, 20, 3), (5, 16, 4)
    ]
    
    for L, B, P in candidates:
        score = evaluate_with_timeout(inp, L, B, P, timeout=10)
        if score < best_score:
            best_score = score
            best_params = (L, B, P)
    
    return (*best_params, best_score)

# ã‚°ãƒ­ãƒ¼ãƒãƒ«é–¢æ•°ã¨ã—ã¦å®šç¾©ï¼ˆpickleå¯èƒ½ã«ã™ã‚‹ãŸã‚ï¼‰
def process_single_file(file_path):
    """1ã¤ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã™ã‚‹é–¢æ•°"""
    try:
        with open(file_path, 'r') as f:
            inp = f.read().strip()
        
        features = extract_feature(inp)
        L, B, P, score = find_best_params_lite(inp)
        return features, (L, B, P), score, os.path.basename(file_path)
    except Exception as e:
        print(f"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«{file_path}ã§ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def create_lite_dataset():
    """è»½é‡ç‰ˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆï¼ˆä¿®æ­£ç‰ˆï¼‰"""
    print("ğŸ“Š è»½é‡ç‰ˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆä¸­...")
    
    # å®Ÿãƒ‡ãƒ¼ã‚¿ã®æ•°ã‚’ç¢ºèª
    real_data_files = sorted(glob.glob(os.path.join(INPUT_DIR, "*.txt")))
    N_REAL_DATA = len(real_data_files)
    
    print(f"   ç™ºè¦‹ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿: {N_REAL_DATA}å€‹")
    print(f"   ä½¿ç”¨ãƒ‡ãƒ¼ã‚¿: {min(LITE_SAMPLE_SIZE, N_REAL_DATA)}å€‹")
    
    if N_REAL_DATA == 0:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {INPUT_DIR} ã«.txtãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return None, None, None
    
    # æœ€åˆã®nå€‹ã®ã¿ä½¿ç”¨
    selected_files = real_data_files[:min(LITE_SAMPLE_SIZE, N_REAL_DATA)]
    print(f"âœ… {len(selected_files)}å€‹ã®ãƒ‡ãƒ¼ã‚¿ã§è»½é‡å­¦ç¿’")
    
    # ä¸¦åˆ—å‡¦ç†ï¼ˆä¿®æ­£ç‰ˆï¼‰
    with mp.Pool(MAX_WORKERS) as pool:
        results = list(tqdm(
            pool.imap(process_single_file, selected_files), 
            total=len(selected_files),
            desc="è»½é‡å‡¦ç†"
        ))
    
    valid_results = [r for r in results if r is not None]
    print(f"âœ… æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿: {len(valid_results)}/{len(selected_files)}")
    
    if len(valid_results) < 5:
        print("âš ï¸ æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã™ãã¾ã™ã€‚")
        return None, None, None
    
    X = np.vstack([r[0] for r in valid_results])
    params = np.array([r[1] for r in valid_results])
    scores = np.array([r[2] for r in valid_results])
    filenames = [r[3] for r in valid_results]
    
    print(f"ğŸ“ˆ ã‚¹ã‚³ã‚¢çµ±è¨ˆ: å¹³å‡={np.mean(scores):.1f}, æœ€å°={np.min(scores):.1f}")
    print(f"ğŸ“ˆ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åˆ†å¸ƒ:")
    print(f"   L: {np.mean(params[:, 0]):.1f} Â± {np.std(params[:, 0]):.1f}")
    print(f"   B: {np.mean(params[:, 1]):.1f} Â± {np.std(params[:, 1]):.1f}")
    print(f"   P: {np.mean(params[:, 2]):.1f} Â± {np.std(params[:, 2]):.1f}")
    
    return X, params, scores

def train_lite_models(X, Y):
    """è»½é‡ç‰ˆãƒ¢ãƒ‡ãƒ«å­¦ç¿’"""
    print("ğŸ¤– è»½é‡ç‰ˆãƒ¢ãƒ‡ãƒ«è¨“ç·´ä¸­...")
    
    lgb_params = {
        'objective': 'regression',
        'metric': 'rmse',
        'boosting_type': 'gbdt',
        'num_leaves': 15,  # è»½é‡åŒ–
        'learning_rate': 0.1,
        'verbose': -1,
        'random_state': 42
    }
    
    models = []
    param_names = ['L (lookahead)', 'B (beam)', 'P (palette)']
    
    for i, name in enumerate(param_names):
        print(f"  ğŸ“š {name} ãƒ¢ãƒ‡ãƒ«è¨“ç·´ä¸­...")
        
        model = lgb.LGBMRegressor(
            **lgb_params,
            n_estimators=50  # è»½é‡åŒ–
        )
        
        model.fit(X, Y[:, i])
        models.append(model)
        
        pred = model.predict(X)
        mae = np.mean(np.abs(pred - Y[:, i]))
        print(f"    âœ… MAE: {mae:.3f}")
    
    return models

def save_lite_models(models):
    """è»½é‡ç‰ˆãƒ¢ãƒ‡ãƒ«ä¿å­˜"""
    print("ğŸ’¾ è»½é‡ç‰ˆãƒ¢ãƒ‡ãƒ«ä¿å­˜ä¸­...")
    
    model_data = []
    for model in models:
        model_json = model.booster_.dump_model()
        model_data.append(model_json)
    
    with open(MODEL_JSON, 'w') as f:
        json.dump(model_data, f, indent=2)
    
    print(f"âœ… è»½é‡ç‰ˆãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†: {MODEL_JSON}")

def test_single_prediction():
    """è»½é‡ãƒ†ã‚¹ãƒˆç”¨ã®å˜ä¸€äºˆæ¸¬"""
    print("ğŸ§ª å˜ä¸€ã‚±ãƒ¼ã‚¹å‹•ä½œãƒ†ã‚¹ãƒˆ...")
    
    # æœ€åˆã®ãƒ•ã‚¡ã‚¤ãƒ«ã§å‹•ä½œãƒ†ã‚¹ãƒˆ
    real_data_files = sorted(glob.glob(os.path.join(INPUT_DIR, "*.txt")))
    if not real_data_files:
        print("âŒ ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãªã—")
        return False
    
    try:
        with open(real_data_files[0], 'r') as f:
            test_inp = f.read().strip()
        
        print(f"  ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«: {os.path.basename(real_data_files[0])}")
        
        # ç‰¹å¾´é‡æŠ½å‡ºãƒ†ã‚¹ãƒˆ
        features = extract_feature(test_inp)
        print(f"  ç‰¹å¾´é‡æ¬¡å…ƒ: {len(features)}")
        print(f"  åŸºæœ¬ç‰¹å¾´é‡: K={features[0]}, H={features[1]}, T={features[2]}, D={features[3]}")
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢ãƒ†ã‚¹ãƒˆ
        L, B, P, score = find_best_params_lite(test_inp)
        print(f"  æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: L={L}, B={B}, P={P}, ã‚¹ã‚³ã‚¢={score}")
        
        return True
    except Exception as e:
        print(f"âŒ ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        return False

def main():
    print("ğŸ§ª AtCoder AHC048 è»½é‡ç‰ˆå­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ï¼ˆä¿®æ­£ç‰ˆï¼‰")
    print("=" * 60)
    
    if not os.path.exists(INPUT_DIR):
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {INPUT_DIR} ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return
    
    if not os.path.exists(SOLVER_BIN):
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {SOLVER_BIN} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return
    
    # äº‹å‰ãƒ†ã‚¹ãƒˆ
    if not test_single_prediction():
        print("âŒ äº‹å‰ãƒ†ã‚¹ãƒˆå¤±æ•—")
        return
    
    start_time = time.time()
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
    X, params, scores = create_lite_dataset()
    if X is None:
        return
    
    print(f"ğŸ“Š ç‰¹å¾´é‡æ¬¡å…ƒ: {X.shape[1]}")
    print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿æ•°: {X.shape[0]}")
    
    # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
    models = train_lite_models(X, params)
    save_lite_models(models)
    
    elapsed = time.time() - start_time
    print(f"â±ï¸ è»½é‡ç‰ˆå­¦ç¿’å®Œäº†: {elapsed:.1f}ç§’")
    print(f"ğŸ§ª ãƒ†ã‚¹ãƒˆç”¨{MODEL_JSON}ãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸï¼")
    
    print("\nâœ… å‹•ä½œç¢ºèªæˆåŠŸï¼æ¬¡ã¯:")
    print("   python gen_header_advanced.py  # C++ãƒ˜ãƒƒãƒ€ç”Ÿæˆ")
    print("   python train_params.py         # æœ¬æ ¼ç‰ˆå­¦ç¿’ï¼ˆ500å€‹ï¼‰")

if __name__ == "__main__":
    main() 