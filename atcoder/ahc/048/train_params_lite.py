#!/usr/bin/env python3
"""
AtCoder AHC048 - è»½é‡ç‰ˆå­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ï¼ˆå‹•ä½œç¢ºèªç”¨ï¼‰
30å€‹ç¨‹åº¦ã®ãƒ‡ãƒ¼ã‚¿ã§å‹•ä½œãƒ†ã‚¹ãƒˆã‚’è¡Œã†

Usage:
    python train_params_lite.py
    â†’ lgbm_params.json ãŒç”Ÿæˆã•ã‚Œã‚‹
"""

import json, subprocess, random, pathlib, tempfile, shutil, multiprocessing as mp
import numpy as np
import lightgbm as lgb
from tqdm import tqdm
import math
import time
import os
import sys
import glob

# è»½é‡è¨­å®š
SOLVER_BIN = "./solver_eval"
MODEL_JSON = "lgbm_params.json"
INPUT_DIR = "./input"
MAX_WORKERS = min(4, mp.cpu_count())

# è»½é‡ç‰ˆè¨­å®š
LITE_SAMPLE_SIZE = 30                  # 30å€‹ã®ã¿ã§ãƒ†ã‚¹ãƒˆ
QUICK_EVALUATION = True                # é«˜é€Ÿè©•ä¾¡ãƒ¢ãƒ¼ãƒ‰

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢ç¯„å›²ï¼ˆè»½é‡ç‰ˆï¼‰
LOOKAHEAD_RANGE = [1, 2, 3, 4, 5]
BEAM_RANGE = [4, 8, 12, 16, 20]
PALETTE_RANGE = [0, 1, 2, 3, 4]

# å®Ÿãƒ‡ãƒ¼ã‚¿ã®æ•°ã‚’ç¢ºèª
real_data_files = sorted(glob.glob(os.path.join(INPUT_DIR, "*.txt")))
N_REAL_DATA = len(real_data_files)

print(f"ğŸ§ª è»½é‡ç‰ˆå­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ï¼ˆå‹•ä½œç¢ºèªç”¨ï¼‰")
print(f"   ç™ºè¦‹ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿: {N_REAL_DATA}å€‹")
print(f"   ä½¿ç”¨ãƒ‡ãƒ¼ã‚¿: {min(LITE_SAMPLE_SIZE, N_REAL_DATA)}å€‹")

def extract_feature(inp: str) -> np.ndarray:
    """åŸºæœ¬çš„ãªç‰¹å¾´é‡ã‚’æŠ½å‡º"""
    lines = inp.strip().split('\n')
    N, K, H, T, D = map(int, lines[0].split())
    
    # æ‰€æœ‰è‰²ã®çµ±è¨ˆé‡
    own_colors = []
    for i in range(1, K + 1):
        r, g, b = map(float, lines[i].split())
        own_colors.append((r, g, b))
    
    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè‰²ã®çµ±è¨ˆé‡  
    target_colors = []
    for i in range(K + 1, K + 1 + H):
        r, g, b = map(float, lines[i].split())
        target_colors.append((r, g, b))
    
    # åŸºæœ¬ç‰¹å¾´é‡ã®ã¿ï¼ˆ17æ¬¡å…ƒï¼‰
    features = [K, H, T, D]
    
    # æ‰€æœ‰è‰²ã®åˆ†æ•£ãƒ»ç¯„å›²
    own_arr = np.array(own_colors)
    features.extend([
        np.var(own_arr[:, 0]),   # Råˆ†æ•£
        np.var(own_arr[:, 1]),   # Gåˆ†æ•£ 
        np.var(own_arr[:, 2]),   # Båˆ†æ•£
        np.ptp(own_arr[:, 0]),   # Rç¯„å›²
        np.ptp(own_arr[:, 1]),   # Gç¯„å›²
        np.ptp(own_arr[:, 2]),   # Bç¯„å›²
    ])
    
    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè‰²ã®çµ±è¨ˆé‡
    target_arr = np.array(target_colors)
    features.extend([
        np.var(target_arr[:, 0]),   # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆRåˆ†æ•£
        np.var(target_arr[:, 1]),   # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆGåˆ†æ•£
        np.var(target_arr[:, 2]),   # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆBåˆ†æ•£
    ])
    
    # è‰²ç©ºé–“ã‚«ãƒãƒ¼ç‡
    min_dists = []
    for tr, tg, tb in target_colors:
        min_dist = min(((tr-or_)**2 + (tg-og)**2 + (tb-ob)**2)**0.5 
                      for or_, og, ob in own_colors)
        min_dists.append(min_dist)
    features.append(np.mean(min_dists))  # å¹³å‡æœ€å°è·é›¢
    features.append(np.std(min_dists))   # è·é›¢ã®æ¨™æº–åå·®
    
    # ã‚³ã‚¹ãƒˆåŠ¹ç‡æŒ‡æ¨™
    features.append(T / H)               # ã‚¿ãƒ¼ãƒ³/ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæ¯”
    features.append(D / T)               # ã‚³ã‚¹ãƒˆå¯†åº¦
    
    return np.array(features, dtype=np.float32)

def evaluate_with_timeout(inp: str, L: int, B: int, P: int, timeout: int = 15) -> float:
    """è»½é‡ç‰ˆè©•ä¾¡é–¢æ•°"""
    try:
        with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:
            f.write(inp)
            input_file = f.name
        
        cmd = [SOLVER_BIN, str(L), str(B), str(P)]
        
        with open(input_file, 'r') as f:
            result = subprocess.run(
                cmd, stdin=f, stdout=subprocess.PIPE, stderr=subprocess.PIPE,
                timeout=timeout, text=True
            )
        
        if result.returncode == 0 and result.stderr:
            try:
                score = float(result.stderr.strip().split()[-1])
                return score
            except:
                pass
        
        return 1e9
        
    except subprocess.TimeoutExpired:
        return 1e9
    except Exception as e:
        print(f"âš ï¸ è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
        return 1e9
    finally:
        if 'input_file' in locals():
            try:
                os.unlink(input_file)
            except:
                pass

def find_best_params_lite(inp: str) -> tuple[int, int, int, float]:
    """è»½é‡ç‰ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢"""
    best_score = 1e9
    best_params = (2, 8, 0)
    
    # ç²—ã„æ¢ç´¢ã®ã¿ï¼ˆé«˜é€ŸåŒ–ï¼‰
    candidates = [
        (1, 4, 0), (2, 8, 0), (2, 12, 1), 
        (3, 16, 2), (4, 20, 3), (5, 16, 4)
    ]
    
    for L, B, P in candidates:
        score = evaluate_with_timeout(inp, L, B, P, timeout=10)
        if score < best_score:
            best_score = score
            best_params = (L, B, P)
    
    return (*best_params, best_score)

def create_lite_dataset():
    """è»½é‡ç‰ˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ"""
    print("ğŸ“Š è»½é‡ç‰ˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆä¸­...")
    
    if N_REAL_DATA == 0:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {INPUT_DIR} ã«.txtãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return None, None, None
    
    # æœ€åˆã®nå€‹ã®ã¿ä½¿ç”¨
    selected_files = real_data_files[:min(LITE_SAMPLE_SIZE, N_REAL_DATA)]
    print(f"âœ… {len(selected_files)}å€‹ã®ãƒ‡ãƒ¼ã‚¿ã§è»½é‡å­¦ç¿’")
    
    def worker(file_path):
        try:
            with open(file_path, 'r') as f:
                inp = f.read().strip()
            
            features = extract_feature(inp)
            L, B, P, score = find_best_params_lite(inp)
            return features, (L, B, P), score
        except Exception as e:
            print(f"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«{file_path}ã§ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    with mp.Pool(MAX_WORKERS) as pool:
        results = list(tqdm(
            pool.imap(worker, selected_files), 
            total=len(selected_files),
            desc="è»½é‡å‡¦ç†"
        ))
    
    valid_results = [r for r in results if r is not None]
    print(f"âœ… æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿: {len(valid_results)}/{len(selected_files)}")
    
    if len(valid_results) < 5:
        print("âš ï¸ æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã™ãã¾ã™ã€‚")
        return None, None, None
    
    X = np.vstack([r[0] for r in valid_results])
    params = np.array([r[1] for r in valid_results])
    scores = np.array([r[2] for r in valid_results])
    
    print(f"ğŸ“ˆ ã‚¹ã‚³ã‚¢çµ±è¨ˆ: å¹³å‡={np.mean(scores):.1f}, æœ€å°={np.min(scores):.1f}")
    
    return X, params, scores

def train_lite_models(X, Y):
    """è»½é‡ç‰ˆãƒ¢ãƒ‡ãƒ«å­¦ç¿’"""
    print("ğŸ¤– è»½é‡ç‰ˆãƒ¢ãƒ‡ãƒ«è¨“ç·´ä¸­...")
    
    lgb_params = {
        'objective': 'regression',
        'metric': 'rmse',
        'boosting_type': 'gbdt',
        'num_leaves': 15,  # è»½é‡åŒ–
        'learning_rate': 0.1,
        'verbose': -1,
        'random_state': 42
    }
    
    models = []
    param_names = ['L (lookahead)', 'B (beam)', 'P (palette)']
    
    for i, name in enumerate(param_names):
        print(f"  ğŸ“š {name} ãƒ¢ãƒ‡ãƒ«è¨“ç·´ä¸­...")
        
        model = lgb.LGBMRegressor(
            **lgb_params,
            n_estimators=50  # è»½é‡åŒ–
        )
        
        model.fit(X, Y[:, i])
        models.append(model)
        
        pred = model.predict(X)
        mae = np.mean(np.abs(pred - Y[:, i]))
        print(f"    âœ… MAE: {mae:.3f}")
    
    return models

def save_lite_models(models):
    """è»½é‡ç‰ˆãƒ¢ãƒ‡ãƒ«ä¿å­˜"""
    print("ğŸ’¾ è»½é‡ç‰ˆãƒ¢ãƒ‡ãƒ«ä¿å­˜ä¸­...")
    
    model_data = []
    for model in models:
        model_json = model.booster_.dump_model()
        model_data.append(model_json)
    
    with open(MODEL_JSON, 'w') as f:
        json.dump(model_data, f, indent=2)
    
    print(f"âœ… è»½é‡ç‰ˆãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†: {MODEL_JSON}")

def main():
    print("ğŸ§ª AtCoder AHC048 è»½é‡ç‰ˆå­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ï¼ˆå‹•ä½œç¢ºèªç”¨ï¼‰")
    print("=" * 60)
    
    if not os.path.exists(INPUT_DIR):
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {INPUT_DIR} ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return
    
    if not os.path.exists(SOLVER_BIN):
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {SOLVER_BIN} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return
    
    start_time = time.time()
    
    X, params, scores = create_lite_dataset()
    if X is None:
        return
    
    print(f"ğŸ“Š ç‰¹å¾´é‡æ¬¡å…ƒ: {X.shape[1]}")
    print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿æ•°: {X.shape[0]}")
    
    models = train_lite_models(X, params)
    save_lite_models(models)
    
    elapsed = time.time() - start_time
    print(f"â±ï¸ è»½é‡ç‰ˆå­¦ç¿’å®Œäº†: {elapsed:.1f}ç§’")
    print(f"ğŸ§ª ãƒ†ã‚¹ãƒˆç”¨{MODEL_JSON}ãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸï¼")
    
    print("\nâœ… å‹•ä½œç¢ºèªæˆåŠŸï¼æ¬¡ã¯æœ¬æ ¼ç‰ˆ:")
    print("   python train_params.py")

if __name__ == "__main__":
    main() 