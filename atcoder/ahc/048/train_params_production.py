#!/usr/bin/env python3
"""
AtCoder AHC048 - æœ¬æ ¼ç‰ˆå¤§é‡ãƒ‡ãƒ¼ã‚¿å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ 
500å€‹ã®å®Ÿãƒ‡ãƒ¼ã‚¿ã§é«˜ç²¾åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ã‚’å­¦ç¿’ã™ã‚‹

Usage:
    python train_params_production.py
    â†’ lgbm_params_production.json ãŒç”Ÿæˆã•ã‚Œã‚‹
"""

import json, subprocess, random, pathlib, tempfile, shutil, multiprocessing as mp
import numpy as np
import lightgbm as lgb
from tqdm import tqdm
import math
import time
import os
import sys
import glob
from sklearn.model_selection import train_test_split
from sklearn.cluster import KMeans
import pickle

# æœ¬æ ¼ç‰ˆè¨­å®š
SOLVER_BIN = "./solver_eval"
MODEL_JSON = "lgbm_params_production.json"
INPUT_DIR = "./input"
CACHE_DIR = "./cache_production"
MAX_WORKERS = min(8, mp.cpu_count())

# æœ¬æ ¼ç‰ˆå­¦ç¿’è¨­å®š
PRODUCTION_SAMPLE_SIZE = 250         # 500å€‹ã‹ã‚‰250å€‹ã‚’ä»£è¡¨é¸æŠ
USE_SMART_SAMPLING = True           # ä»£è¡¨æ€§ã®ã‚ã‚‹ãƒ‡ãƒ¼ã‚¿é¸æŠ
USE_CACHE = True                    # è¨ˆç®—çµæœã‚­ãƒ£ãƒƒã‚·ãƒ¥
ENABLE_ENSEMBLE = True              # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’
ENABLE_ADVANCED_FEATURES = True     # é«˜åº¦ç‰¹å¾´é‡

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢ç¯„å›²ï¼ˆæœ¬æ ¼ç‰ˆï¼‰
LOOKAHEAD_RANGE = [1, 2, 3, 4, 5]      
BEAM_RANGE = [4, 6, 8, 12, 16, 20, 24]  
PALETTE_RANGE = [0, 1, 2, 3, 4]        

print(f"ğŸš€ AtCoder AHC048 æœ¬æ ¼ç‰ˆå¤§é‡ãƒ‡ãƒ¼ã‚¿å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ")
print(f"   ä¸¦åˆ—å‡¦ç†: {MAX_WORKERS}ã‚³ã‚¢")
print(f"   ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º: {PRODUCTION_SAMPLE_SIZE}")

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
if USE_CACHE:
    os.makedirs(CACHE_DIR, exist_ok=True)

def extract_advanced_features(inp: str) -> np.ndarray:
    """é«˜åº¦ãªç‰¹å¾´é‡ã‚’æŠ½å‡ºï¼ˆ22æ¬¡å…ƒï¼‰"""
    lines = inp.strip().split('\n')
    N, K, H, T, D = map(int, lines[0].split())
    
    # æ‰€æœ‰è‰²ã®çµ±è¨ˆé‡
    own_colors = []
    for i in range(1, K + 1):
        r, g, b = map(float, lines[i].split())
        own_colors.append((r, g, b))
    
    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè‰²ã®çµ±è¨ˆé‡  
    target_colors = []
    for i in range(K + 1, K + 1 + H):
        r, g, b = map(float, lines[i].split())
        target_colors.append((r, g, b))
    
    # åŸºæœ¬ç‰¹å¾´é‡ï¼ˆ4æ¬¡å…ƒï¼‰
    features = [K, H, T, D]
    
    # æ‰€æœ‰è‰²ã®é«˜åº¦çµ±è¨ˆé‡ï¼ˆ6æ¬¡å…ƒï¼‰
    own_arr = np.array(own_colors)
    features.extend([
        np.var(own_arr[:, 0]),   # Råˆ†æ•£
        np.var(own_arr[:, 1]),   # Gåˆ†æ•£ 
        np.var(own_arr[:, 2]),   # Båˆ†æ•£
        np.ptp(own_arr[:, 0]),   # Rç¯„å›²
        np.ptp(own_arr[:, 1]),   # Gç¯„å›²
        np.ptp(own_arr[:, 2]),   # Bç¯„å›²
    ])
    
    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè‰²ã®çµ±è¨ˆé‡ï¼ˆ3æ¬¡å…ƒï¼‰
    target_arr = np.array(target_colors)
    features.extend([
        np.var(target_arr[:, 0]),   # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆRåˆ†æ•£
        np.var(target_arr[:, 1]),   # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆGåˆ†æ•£
        np.var(target_arr[:, 2]),   # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆBåˆ†æ•£
    ])
    
    # è‰²ç©ºé–“ã‚«ãƒãƒ¼ç‡ï¼ˆ2æ¬¡å…ƒï¼‰
    min_dists = []
    for tr, tg, tb in target_colors:
        min_dist = min(((tr-or_)**2 + (tg-og)**2 + (tb-ob)**2)**0.5 
                      for or_, og, ob in own_colors)
        min_dists.append(min_dist)
    features.append(np.mean(min_dists))  # å¹³å‡æœ€å°è·é›¢
    features.append(np.std(min_dists))   # è·é›¢ã®æ¨™æº–åå·®
    
    # ã‚³ã‚¹ãƒˆåŠ¹ç‡æŒ‡æ¨™ï¼ˆ2æ¬¡å…ƒï¼‰
    features.append(T / H)               # ã‚¿ãƒ¼ãƒ³/ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæ¯”
    features.append(D / T)               # ã‚³ã‚¹ãƒˆå¯†åº¦
    
    if ENABLE_ADVANCED_FEATURES:
        # é«˜åº¦ç‰¹å¾´é‡ï¼ˆ5æ¬¡å…ƒï¼‰
        features.append(K / H)           # è‰²/ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæ¯”
        features.append(np.log(T + 1))   # ãƒ­ã‚°ã‚¿ãƒ¼ãƒ³æ•°
        features.append(np.log(D + 1))   # ãƒ­ã‚°ã‚³ã‚¹ãƒˆ
        
        # è‰²åˆ†å¸ƒã®è¤‡é›‘æ€§
        own_distances = []
        for i in range(len(own_colors)):
            for j in range(i + 1, len(own_colors)):
                dist = ((own_colors[i][0] - own_colors[j][0])**2 + 
                       (own_colors[i][1] - own_colors[j][1])**2 + 
                       (own_colors[i][2] - own_colors[j][2])**2)**0.5
                own_distances.append(dist)
        
        features.append(np.mean(own_distances) if own_distances else 0)  # æ‰€æœ‰è‰²é–“è·é›¢
        features.append(np.std(own_distances) if own_distances else 0)   # è·é›¢åˆ†æ•£
    
    return np.array(features, dtype=np.float32)

def smart_sample_selection(files, target_size):
    """ä»£è¡¨æ€§ã®ã‚ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’é¸æŠ"""
    print(f"ğŸ§  ã‚¹ãƒãƒ¼ãƒˆã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°: {len(files)}å€‹ â†’ {target_size}å€‹")
    
    if len(files) <= target_size:
        return files
    
    # ã¾ãš100å€‹ã§ç‰¹å¾´é‡åˆ†æ
    sample_files = random.sample(files, min(100, len(files)))
    features_list = []
    valid_files = []
    
    for file_path in tqdm(sample_files, desc="ç‰¹å¾´é‡åˆ†æ"):
        try:
            with open(file_path, 'r') as f:
                content = f.read().strip()
            feature = extract_advanced_features(content)
            features_list.append(feature)
            valid_files.append(file_path)
        except:
            continue
    
    if len(features_list) < 20:
        print("âš ï¸ åˆ†æãƒ‡ãƒ¼ã‚¿ä¸è¶³ã€ãƒ©ãƒ³ãƒ€ãƒ é¸æŠã«åˆ‡ã‚Šæ›¿ãˆ")
        return random.sample(files, target_size)
    
    # K-meansã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã§å¤šæ§˜æ€§ç¢ºä¿
    n_clusters = min(target_size // 5, len(features_list) // 3)
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    cluster_labels = kmeans.fit_predict(features_list)
    
    selected_files = set()
    
    # å„ã‚¯ãƒ©ã‚¹ã‚¿ã‹ã‚‰ä»£è¡¨é¸æŠ
    for cluster_id in range(n_clusters):
        cluster_files = [valid_files[i] for i, label in enumerate(cluster_labels) if label == cluster_id]
        cluster_size = min(len(cluster_files), target_size // n_clusters + 1)
        selected_files.update(random.sample(cluster_files, cluster_size))
    
    # æ®‹ã‚Šã‚’ãƒ©ãƒ³ãƒ€ãƒ é¸æŠã§åŸ‹ã‚ã‚‹
    remaining_files = [f for f in files if f not in selected_files]
    need_more = target_size - len(selected_files)
    if need_more > 0:
        selected_files.update(random.sample(remaining_files, min(need_more, len(remaining_files))))
    
    result = list(selected_files)[:target_size]
    print(f"âœ… å¤šæ§˜æ€§ã‚’è€ƒæ…®ã—ãŸ{len(result)}å€‹ã‚’é¸æŠ")
    return result

def evaluate_with_cache(inp: str, L: int, B: int, P: int) -> float:
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãè©•ä¾¡é–¢æ•°"""
    cache_key = f"{hash(inp) % 1000000}_{L}_{B}_{P}"
    cache_file = os.path.join(CACHE_DIR, f"{cache_key}.txt") if USE_CACHE else None
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç¢ºèª
    if cache_file and os.path.exists(cache_file):
        try:
            with open(cache_file, 'r') as f:
                return float(f.read().strip())
        except:
            pass
    
    # å®Ÿéš›ã®è©•ä¾¡
    try:
        with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:
            f.write(inp)
            input_file = f.name
        
        cmd = [SOLVER_BIN, str(L), str(B), str(P)]
        
        with open(input_file, 'r') as f:
            result = subprocess.run(
                cmd, stdin=f, stdout=subprocess.PIPE, stderr=subprocess.PIPE,
                timeout=25, text=True
            )
        
        if result.returncode == 0 and result.stderr:
            try:
                score = float(result.stderr.strip().split()[-1])
                
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜
                if cache_file:
                    try:
                        with open(cache_file, 'w') as f:
                            f.write(str(score))
                    except:
                        pass
                
                return score
            except:
                pass
        
        return 1e9
        
    except subprocess.TimeoutExpired:
        return 1e9
    except Exception as e:
        return 1e9
    finally:
        if 'input_file' in locals():
            try:
                os.unlink(input_file)
            except:
                pass

def find_best_params_production(inp: str) -> tuple[int, int, int, float]:
    """æœ¬æ ¼ç‰ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢ï¼ˆã‚ˆã‚Šç´°ã‹ãï¼‰"""
    best_score = 1e9
    best_params = (2, 8, 0)
    evaluations = 0
    max_evaluations = 20  # æœ¬æ ¼ç‰ˆã§ã¯ç²¾å¯†ã«
    
    # ãƒ•ã‚§ãƒ¼ã‚º1: åŸºæœ¬å€™è£œ
    phase1_candidates = [
        (1, 4, 0), (1, 8, 1), (2, 8, 0), (2, 12, 2), 
        (3, 16, 1), (4, 20, 3), (5, 24, 4),
        (2, 6, 1), (3, 12, 0), (4, 16, 2)
    ]
    
    promising = []
    
    for L, B, P in phase1_candidates:
        if evaluations >= max_evaluations // 2:
            break
        score = evaluate_with_cache(inp, L, B, P)
        evaluations += 1
        
        if score < best_score:
            best_score = score
            best_params = (L, B, P)
        
        promising.append((L, B, P, score))
    
    # ãƒ•ã‚§ãƒ¼ã‚º2: ä¸Šä½å€™è£œå‘¨è¾ºã‚’ç²¾å¯†æ¢ç´¢
    promising.sort(key=lambda x: x[3])
    top_3 = promising[:3]
    
    for base_L, base_B, base_P, _ in top_3:
        for dL in [-1, 0, 1]:
            for dB in [-4, 0, 4]:
                if evaluations >= max_evaluations:
                    break
                
                L = max(1, min(5, base_L + dL))
                B = max(4, min(24, base_B + dB))
                P = base_P  # ãƒ‘ãƒ¬ãƒƒãƒˆã¯å›ºå®š
                
                if B not in BEAM_RANGE:
                    continue
                
                # æ—¢è©•ä¾¡ã‚’ã‚¹ã‚­ãƒƒãƒ—
                if (L, B, P) in [(c[0], c[1], c[2]) for c in promising]:
                    continue
                
                score = evaluate_with_cache(inp, L, B, P)
                evaluations += 1
                
                if score < best_score:
                    best_score = score
                    best_params = (L, B, P)
    
    return (*best_params, best_score)

def process_single_file_production(file_path):
    """æœ¬æ ¼ç‰ˆãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†"""
    try:
        with open(file_path, 'r') as f:
            inp = f.read().strip()
        
        features = extract_advanced_features(inp)
        L, B, P, score = find_best_params_production(inp)
        return features, (L, B, P), score, os.path.basename(file_path)
    except Exception as e:
        print(f"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«{file_path}ã§ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def create_production_dataset():
    """æœ¬æ ¼ç‰ˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ"""
    print("ğŸ­ æœ¬æ ¼ç‰ˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆä¸­...")
    
    # å®Ÿãƒ‡ãƒ¼ã‚¿ã®æ•°ã‚’ç¢ºèª
    real_data_files = sorted(glob.glob(os.path.join(INPUT_DIR, "*.txt")))
    N_REAL_DATA = len(real_data_files)
    
    print(f"   ç™ºè¦‹ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿: {N_REAL_DATA}å€‹")
    
    if N_REAL_DATA == 0:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {INPUT_DIR} ã«.txtãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return None, None, None
    
    # ã‚¹ãƒãƒ¼ãƒˆã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    if USE_SMART_SAMPLING and N_REAL_DATA > PRODUCTION_SAMPLE_SIZE:
        selected_files = smart_sample_selection(real_data_files, PRODUCTION_SAMPLE_SIZE)
    else:
        selected_files = real_data_files[:PRODUCTION_SAMPLE_SIZE]
    
    print(f"âœ… {len(selected_files)}å€‹ã®ãƒ‡ãƒ¼ã‚¿ã§æœ¬æ ¼å­¦ç¿’")
    
    # ä¸¦åˆ—å‡¦ç†
    with mp.Pool(MAX_WORKERS) as pool:
        results = list(tqdm(
            pool.imap(process_single_file_production, selected_files), 
            total=len(selected_files),
            desc="æœ¬æ ¼å‡¦ç†"
        ))
    
    valid_results = [r for r in results if r is not None]
    print(f"âœ… æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿: {len(valid_results)}/{len(selected_files)}")
    
    if len(valid_results) < 20:
        print("âš ï¸ æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã™ãã¾ã™ã€‚")
        return None, None, None
    
    X = np.vstack([r[0] for r in valid_results])
    params = np.array([r[1] for r in valid_results])
    scores = np.array([r[2] for r in valid_results])
    filenames = [r[3] for r in valid_results]
    
    print(f"ğŸ“ˆ ã‚¹ã‚³ã‚¢çµ±è¨ˆ: å¹³å‡={np.mean(scores):.1f}, æœ€å°={np.min(scores):.1f}, æœ€å¤§={np.max(scores):.1f}")
    print(f"ğŸ“ˆ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åˆ†å¸ƒ:")
    print(f"   L: {np.mean(params[:, 0]):.2f} Â± {np.std(params[:, 0]):.2f} (ç¯„å›²: {np.min(params[:, 0])}-{np.max(params[:, 0])})")
    print(f"   B: {np.mean(params[:, 1]):.2f} Â± {np.std(params[:, 1]):.2f} (ç¯„å›²: {np.min(params[:, 1])}-{np.max(params[:, 1])})")
    print(f"   P: {np.mean(params[:, 2]):.2f} Â± {np.std(params[:, 2]):.2f} (ç¯„å›²: {np.min(params[:, 2])}-{np.max(params[:, 2])})")
    
    return X, params, scores

def train_production_models(X, Y):
    """æœ¬æ ¼ç‰ˆã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«å­¦ç¿’"""
    print("ğŸ“ æœ¬æ ¼ç‰ˆã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«è¨“ç·´ä¸­...")
    
    # è¨“ç·´ãƒ»æ¤œè¨¼åˆ†å‰²
    X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=42)
    
    # LightGBMé«˜ç²¾åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    lgb_params = {
        'objective': 'regression',
        'metric': 'rmse',
        'boosting_type': 'gbdt',
        'num_leaves': 127,
        'learning_rate': 0.05,
        'feature_fraction': 0.8,
        'bagging_fraction': 0.8,
        'bagging_freq': 5,
        'verbose': -1,
        'random_state': 42,
        'reg_alpha': 0.1,
        'reg_lambda': 0.1
    }
    
    models = []
    param_names = ['L (lookahead)', 'B (beam)', 'P (palette)']
    
    for i, name in enumerate(param_names):
        print(f"  ğŸ“š {name} ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«è¨“ç·´ä¸­...")
        
        if ENABLE_ENSEMBLE:
            # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«: 3ã¤ã®ãƒ¢ãƒ‡ãƒ«
            ensemble = []
            for seed in [42, 123, 456]:
                lgb_params_copy = lgb_params.copy()
                lgb_params_copy['random_state'] = seed
                model = lgb.LGBMRegressor(
                    **lgb_params_copy,
                    n_estimators=500  # ã‚ˆã‚Šå¤šãã®æœ¨
                )
                model.fit(X_train, Y_train[:, i])
                ensemble.append(model)
            
            # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬è©•ä¾¡
            pred_ensemble = np.mean([m.predict(X_val) for m in ensemble], axis=0)
            mae = np.mean(np.abs(pred_ensemble - Y_val[:, i]))
            rmse = np.sqrt(np.mean((pred_ensemble - Y_val[:, i])**2))
            
            models.append(ensemble)
            print(f"    âœ… MAE: {mae:.3f}, RMSE: {rmse:.3f}")
        else:
            model = lgb.LGBMRegressor(**lgb_params, n_estimators=500)
            model.fit(X_train, Y_train[:, i])
            
            pred = model.predict(X_val)
            mae = np.mean(np.abs(pred - Y_val[:, i]))
            models.append([model])
            print(f"    âœ… MAE: {mae:.3f}")
    
    return models

def save_production_models(models):
    """æœ¬æ ¼ç‰ˆãƒ¢ãƒ‡ãƒ«ä¿å­˜"""
    print("ğŸ’¾ æœ¬æ ¼ç‰ˆã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ä¿å­˜ä¸­...")
    
    model_data = []
    for model_ensemble in models:
        if isinstance(model_ensemble, list) and len(model_ensemble) > 1:
            # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã®å ´åˆ
            ensemble_data = []
            for model in model_ensemble:
                model_json = model.booster_.dump_model()
                ensemble_data.append(model_json)
            model_data.append(ensemble_data)
        else:
            # å˜ä¸€ãƒ¢ãƒ‡ãƒ«ã®å ´åˆ
            model = model_ensemble[0] if isinstance(model_ensemble, list) else model_ensemble
            model_json = model.booster_.dump_model()
            model_data.append([model_json])
    
    with open(MODEL_JSON, 'w') as f:
        json.dump(model_data, f, indent=2)
    
    print(f"âœ… æœ¬æ ¼ç‰ˆã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†: {MODEL_JSON}")

def main():
    print("ğŸš€ AtCoder AHC048 æœ¬æ ¼ç‰ˆå¤§é‡ãƒ‡ãƒ¼ã‚¿å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ")
    print("=" * 70)
    
    if not os.path.exists(INPUT_DIR):
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {INPUT_DIR} ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return
    
    if not os.path.exists(SOLVER_BIN):
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {SOLVER_BIN} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return
    
    start_time = time.time()
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
    X, params, scores = create_production_dataset()
    if X is None:
        return
    
    print(f"ğŸ“Š ç‰¹å¾´é‡æ¬¡å…ƒ: {X.shape[1]}")
    print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿æ•°: {X.shape[0]}")
    print(f"ğŸ“Š ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«: {'æœ‰åŠ¹' if ENABLE_ENSEMBLE else 'ç„¡åŠ¹'}")
    
    # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
    models = train_production_models(X, params)
    save_production_models(models)
    
    elapsed = time.time() - start_time
    print(f"â±ï¸ æœ¬æ ¼ç‰ˆå­¦ç¿’å®Œäº†: {elapsed:.1f}ç§’")
    print(f"ğŸ‰ é«˜ç²¾åº¦{MODEL_JSON}ãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸï¼")
    
    print("\nğŸ† æœ¬æ ¼ç‰ˆå®Œæˆï¼æ¬¡ã¯:")
    print("   mv lgbm_params_production.json lgbm_params.json")
    print("   python gen_header_advanced.py")
    print("   g++ -O2 -std=c++20 -o solver_final solver_ml.cpp")

if __name__ == "__main__":
    main() 