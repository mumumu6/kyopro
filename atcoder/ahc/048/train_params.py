#!/usr/bin/env python3
"""
AtCoder Heuristic Contest 048 - å¤§é‡ãƒ‡ãƒ¼ã‚¿å¯¾å¿œè‡ªå‹•ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ 
500å€‹ã®å®Ÿãƒ‡ãƒ¼ã‚¿ã‚’åŠ¹ç‡çš„ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¦æœ€é©ãª(L=å…ˆèª­ã¿, B=ãƒ“ãƒ¼ãƒ å¹…, P=ãƒ‘ãƒ¬ãƒƒãƒˆ)ã‚’å­¦ç¿’ã™ã‚‹

Usage:
    python train_params.py
    â†’ lgbm_params.json ãŒç”Ÿæˆã•ã‚Œã‚‹
"""

import json, subprocess, random, pathlib, tempfile, shutil, multiprocessing as mp
import numpy as np
import lightgbm as lgb
from tqdm import tqdm
import math
import time
import os
import sys
import glob
from sklearn.model_selection import train_test_split
from sklearn.cluster import KMeans
import pickle

# è¨­å®š
SOLVER_BIN = "./solver_eval"           # è©•ä¾¡å°‚ç”¨ã‚½ãƒ«ãƒ (å¾Œã§ä½œæˆ)
MODEL_JSON = "lgbm_params.json"
INPUT_DIR = "./input"                  # å®Ÿãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
CACHE_DIR = "./cache"                  # è¨ˆç®—çµæœã‚­ãƒ£ãƒƒã‚·ãƒ¥ç”¨
MAX_WORKERS = min(12, mp.cpu_count())  # ä¸¦åˆ—æ•°

# å¤§é‡ãƒ‡ãƒ¼ã‚¿å¯¾å¿œæˆ¦ç•¥è¨­å®š
USE_SMART_SAMPLING = True              # ä»£è¡¨çš„ãªãƒ‡ãƒ¼ã‚¿ã®ã¿å­¦ç¿’ã™ã‚‹ã‹
SMART_SAMPLE_SIZE = 200               # ä»£è¡¨ãƒ‡ãƒ¼ã‚¿æ•°ï¼ˆ500å€‹ã‹ã‚‰200å€‹ã‚’é¸æŠï¼‰
USE_CACHE = True                      # è¨ˆç®—çµæœã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ã†ã‹
ENABLE_ADVANCED_FEATURES = True       # é«˜åº¦ãªç‰¹å¾´é‡ã‚’ä½¿ã†ã‹
USE_ENSEMBLE = True                   # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ã™ã‚‹ã‹

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢ç¯„å›²ï¼ˆå¤§é‡ãƒ‡ãƒ¼ã‚¿ãªã®ã§ã‚ˆã‚Šç´°ã‹ãï¼‰
LOOKAHEAD_RANGE = [1, 2, 3, 4, 5]      # å…ˆèª­ã¿æ·±ã•
BEAM_RANGE = [4, 6, 8, 12, 16, 20, 24]  # ãƒ“ãƒ¼ãƒ å¹…ï¼ˆã‚ˆã‚Šç´°ã‹ãï¼‰
PALETTE_RANGE = [0, 1, 2, 3, 4]        # ãƒ‘ãƒ¬ãƒƒãƒˆãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆID

# å®Ÿãƒ‡ãƒ¼ã‚¿ã®æ•°ã‚’ç¢ºèª
real_data_files = sorted(glob.glob(os.path.join(INPUT_DIR, "*.txt")))
N_REAL_DATA = len(real_data_files)

print(f"ğŸ¯ å¤§é‡ãƒ‡ãƒ¼ã‚¿å¯¾å¿œå­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ èµ·å‹•ï¼")
print(f"   ç™ºè¦‹ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿: {N_REAL_DATA}å€‹")
print(f"   ã‚¹ãƒãƒ¼ãƒˆã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°: {'ON' if USE_SMART_SAMPLING else 'OFF'}")
print(f"   ä¸¦åˆ—å‡¦ç†: {MAX_WORKERS}ã‚³ã‚¢")

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
if USE_CACHE:
    os.makedirs(CACHE_DIR, exist_ok=True)

# ------------------ ã‚¹ãƒãƒ¼ãƒˆã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° ------------------
def analyze_data_diversity(files):
    """ãƒ‡ãƒ¼ã‚¿ã®å¤šæ§˜æ€§ã‚’åˆ†æã—ã¦ä»£è¡¨çš„ãªã‚±ãƒ¼ã‚¹ã‚’é¸å‡º"""
    print("ğŸ” ãƒ‡ãƒ¼ã‚¿å¤šæ§˜æ€§åˆ†æä¸­...")
    
    features_for_clustering = []
    file_features = {}
    
    for file_path in tqdm(files[:100], desc="ç‰¹å¾´é‡æŠ½å‡º"):  # å…¨éƒ¨ã¯é‡ã„ã®ã§100å€‹ã§åˆ†æ
        try:
            with open(file_path, 'r') as f:
                content = f.read().strip()
            feature = extract_feature(content)
            features_for_clustering.append(feature)
            file_features[file_path] = feature
        except:
            continue
    
    if len(features_for_clustering) < 10:
        print("âš ï¸ åˆ†æç”¨ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã€‚å…¨ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
        return files
    
    # K-meansã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã§ä»£è¡¨ã‚±ãƒ¼ã‚¹ã‚’é¸å‡º
    n_clusters = min(SMART_SAMPLE_SIZE // 5, len(features_for_clustering) // 2)
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    cluster_labels = kmeans.fit_predict(features_for_clustering)
    
    # å„ã‚¯ãƒ©ã‚¹ã‚¿ã‹ã‚‰ä»£è¡¨çš„ãªã‚±ãƒ¼ã‚¹ã‚’é¸å‡º
    selected_files = []
    for cluster_id in range(n_clusters):
        cluster_files = [files[i] for i, label in enumerate(cluster_labels) if label == cluster_id]
        # ã‚¯ãƒ©ã‚¹ã‚¿ä¸­å¿ƒã«æœ€ã‚‚è¿‘ã„ã‚±ãƒ¼ã‚¹ã‚’é¸ã¶
        cluster_center = kmeans.cluster_centers_[cluster_id]
        best_file = cluster_files[0]
        best_dist = float('inf')
        
        for file_path in cluster_files:
            if file_path in file_features:
                dist = np.linalg.norm(file_features[file_path] - cluster_center)
                if dist < best_dist:
                    best_dist = dist
                    best_file = file_path
        
        selected_files.append(best_file)
        
        # ã‚¯ãƒ©ã‚¹ã‚¿ã‚µã‚¤ã‚ºã«å¿œã˜ã¦è¿½åŠ é¸æŠ
        additional = min(len(cluster_files) - 1, SMART_SAMPLE_SIZE // n_clusters - 1)
        for i in range(additional):
            if i + 1 < len(cluster_files):
                selected_files.append(cluster_files[i + 1])
    
    print(f"âœ… {len(selected_files)}å€‹ã®ä»£è¡¨ã‚±ãƒ¼ã‚¹ã‚’é¸å‡º")
    return selected_files[:SMART_SAMPLE_SIZE]

# ------------------ é«˜åº¦ãªç‰¹å¾´é‡æŠ½å‡º ------------------
def extract_advanced_features(inp: str) -> np.ndarray:
    """é«˜åº¦ãªç‰¹å¾´é‡ã‚’æŠ½å‡ºï¼ˆå¤§é‡ãƒ‡ãƒ¼ã‚¿å¯¾å¿œï¼‰"""
    lines = inp.strip().split('\n')
    N, K, H, T, D = map(int, lines[0].split())
    
    # æ‰€æœ‰è‰²ã®çµ±è¨ˆé‡
    own_colors = []
    for i in range(1, K + 1):
        r, g, b = map(float, lines[i].split())
        own_colors.append((r, g, b))
    
    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè‰²ã®çµ±è¨ˆé‡  
    target_colors = []
    for i in range(K + 1, K + 1 + H):
        r, g, b = map(float, lines[i].split())
        target_colors.append((r, g, b))
    
    # åŸºæœ¬ç‰¹å¾´é‡
    features = [K, H, T, D]
    
    # æ‰€æœ‰è‰²ã®åˆ†æ•£ãƒ»ç¯„å›²
    own_arr = np.array(own_colors)
    features.extend([
        np.var(own_arr[:, 0]),   # Råˆ†æ•£
        np.var(own_arr[:, 1]),   # Gåˆ†æ•£ 
        np.var(own_arr[:, 2]),   # Båˆ†æ•£
        np.ptp(own_arr[:, 0]),   # Rç¯„å›²
        np.ptp(own_arr[:, 1]),   # Gç¯„å›²
        np.ptp(own_arr[:, 2]),   # Bç¯„å›²
    ])
    
    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè‰²ã®çµ±è¨ˆé‡
    target_arr = np.array(target_colors)
    features.extend([
        np.var(target_arr[:, 0]),   # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆRåˆ†æ•£
        np.var(target_arr[:, 1]),   # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆGåˆ†æ•£
        np.var(target_arr[:, 2]),   # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆBåˆ†æ•£
    ])
    
    # è‰²ç©ºé–“ã‚«ãƒãƒ¼ç‡
    min_dists = []
    for tr, tg, tb in target_colors:
        min_dist = min(((tr-or_)**2 + (tg-og)**2 + (tb-ob)**2)**0.5 
                      for or_, og, ob in own_colors)
        min_dists.append(min_dist)
    features.append(np.mean(min_dists))  # å¹³å‡æœ€å°è·é›¢
    features.append(np.std(min_dists))   # è·é›¢ã®æ¨™æº–åå·®
    
    # ã‚³ã‚¹ãƒˆåŠ¹ç‡æŒ‡æ¨™
    features.append(T / H)               # ã‚¿ãƒ¼ãƒ³/ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæ¯”
    features.append(D / T)               # ã‚³ã‚¹ãƒˆå¯†åº¦
    
    if ENABLE_ADVANCED_FEATURES:
        # é«˜åº¦ãªç‰¹å¾´é‡
        features.append(K / H)           # è‰²/ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæ¯”
        features.append(np.log(T + 1))   # ãƒ­ã‚°ã‚¿ãƒ¼ãƒ³æ•°
        features.append(np.log(D + 1))   # ãƒ­ã‚°ã‚³ã‚¹ãƒˆ
        
        # è‰²åˆ†å¸ƒã®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
        own_distances = []
        for i in range(len(own_colors)):
            for j in range(i + 1, len(own_colors)):
                dist = ((own_colors[i][0] - own_colors[j][0])**2 + 
                       (own_colors[i][1] - own_colors[j][1])**2 + 
                       (own_colors[i][2] - own_colors[j][2])**2)**0.5
                own_distances.append(dist)
        
        features.append(np.mean(own_distances) if own_distances else 0)  # æ‰€æœ‰è‰²é–“è·é›¢
        features.append(np.std(own_distances) if own_distances else 0)   # è·é›¢åˆ†æ•£
        
        # é›£æ˜“åº¦æŒ‡æ¨™
        difficulty = np.mean(min_dists) * D / T + K / (H + 1)
        features.append(difficulty)
    
    return np.array(features, dtype=np.float32)

# æ—¢å­˜ã®extract_featureé–¢æ•°ã‚’ç½®ãæ›ãˆ
extract_feature = extract_advanced_features if ENABLE_ADVANCED_FEATURES else extract_feature

# ------------------ å…¥åŠ›ç”Ÿæˆ ------------------
def gen_case(seed: int) -> str:
    """AHC048ã®å•é¡Œå½¢å¼ã«åˆã‚ã›ã¦å…¥åŠ›ã‚’ç”Ÿæˆ"""
    random.seed(seed)
    np.random.seed(seed)
    
    # å•é¡Œãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç”Ÿæˆ (å®Ÿéš›ã®ã‚³ãƒ³ãƒ†ã‚¹ãƒˆåˆ†å¸ƒã«è¿‘ä¼¼)
    N = 20
    K = random.randint(4, 20)
    H = random.randint(800, 1000)
    T = round(5000 * (2**random.uniform(0, 3.5)))  # 5000-40000ç¨‹åº¦
    D = round(10 ** random.uniform(1.5, 3.5))      # 32-3162ç¨‹åº¦
    
    # æ‰€æœ‰è‰²ç”Ÿæˆ (RGBå„æˆåˆ†0-1ã®å®Ÿæ•°)
    own_colors = []
    for _ in range(K):
        # å°‘ã—åã‚Šã‚’æŒãŸã›ã¦ç¾å®Ÿçš„ãªè‰²åˆ†å¸ƒã«ã™ã‚‹
        r = np.clip(np.random.beta(2, 2), 0, 1)
        g = np.clip(np.random.beta(2, 2), 0, 1) 
        b = np.clip(np.random.beta(2, 2), 0, 1)
        own_colors.append((r, g, b))
    
    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè‰²ç”Ÿæˆ
    target_colors = []
    for _ in range(H):
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã¯æ‰€æœ‰è‰²ã‹ã‚‰ä½œã‚Œã‚‹ç¯„å›²ã§ç”Ÿæˆï¼ˆå­¦ç¿’åŠ¹ç‡å‘ä¸Šï¼‰
        if random.random() < 0.7 and len(own_colors) >= 2:
            # 70%ã®ç¢ºç‡ã§æ‰€æœ‰è‰²ã®ç·šå½¢çµåˆã‹ã‚‰ç”Ÿæˆ
            c1, c2 = random.sample(own_colors, 2)
            alpha = random.random()
            r = alpha * c1[0] + (1-alpha) * c2[0]
            g = alpha * c1[1] + (1-alpha) * c2[1]
            b = alpha * c1[2] + (1-alpha) * c2[2]
        else:
            # 30%ã¯å®Œå…¨ãƒ©ãƒ³ãƒ€ãƒ ï¼ˆé›£ã—ã„ã‚±ãƒ¼ã‚¹ï¼‰
            r = random.random()
            g = random.random()
            b = random.random()
        target_colors.append((r, g, b))
    
    # å…¥åŠ›æ–‡å­—åˆ—ç”Ÿæˆ
    lines = [f"{N} {K} {H} {T} {D}"]
    for r, g, b in own_colors:
        lines.append(f"{r:.6f} {g:.6f} {b:.6f}")
    for r, g, b in target_colors:
        lines.append(f"{r:.6f} {g:.6f} {b:.6f}")
    
    return "\n".join(lines)

# ------------------ è©•ä¾¡é–¢æ•° ------------------
def evaluate_with_timeout(inp: str, L: int, B: int, P: int, timeout: int = 30) -> float:
    """æŒ‡å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã‚½ãƒ«ãƒã‚’å®Ÿè¡Œã—ã¦ã‚¹ã‚³ã‚¢ã‚’å–å¾—ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãï¼‰"""
    try:
        with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:
            f.write(inp)
            input_file = f.name
        
        # ã‚½ãƒ«ãƒå®Ÿè¡Œ (L, B, P ã‚’ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã§æ¸¡ã™)
        cmd = [SOLVER_BIN, str(L), str(B), str(P)]
        
        with open(input_file, 'r') as f:
            result = subprocess.run(
                cmd, stdin=f, stdout=subprocess.PIPE, stderr=subprocess.PIPE,
                timeout=timeout, text=True
            )
        
        # ã‚¹ã‚³ã‚¢ã‚’stderrã‹ã‚‰å–å¾—ï¼ˆå®Ÿè£…ã«å¿œã˜ã¦èª¿æ•´ï¼‰
        if result.returncode == 0 and result.stderr:
            try:
                score = float(result.stderr.strip().split()[-1])
                return score
            except:
                pass
        
        # fallback: å·¨å¤§ãªãƒšãƒŠãƒ«ãƒ†ã‚£
        return 1e9
        
    except subprocess.TimeoutExpired:
        return 1e9
    except Exception as e:
        print(f"âš ï¸ è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
        return 1e9
    finally:
        if 'input_file' in locals():
            try:
                os.unlink(input_file)
            except:
                pass

# ------------------ ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆæ¢ç´¢ ------------------
def intelligent_param_search(inp: str, max_evaluations: int = 35) -> tuple[int, int, int, float]:
    """ãƒ™ã‚¤ã‚¸ã‚¢ãƒ³æœ€é©åŒ–é¢¨ã®ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆæ¢ç´¢"""
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
    cache_key = hash(inp) % 1000000
    cache_file = os.path.join(CACHE_DIR, f"cache_{cache_key}.pkl") if USE_CACHE else None
    
    if cache_file and os.path.exists(cache_file):
        try:
            with open(cache_file, 'rb') as f:
                return pickle.load(f)
        except:
            pass
    
    best_score = 1e9
    best_params = (2, 8, 0)
    evaluations = 0
    
    # ãƒ•ã‚§ãƒ¼ã‚º1: ç²—ã„æ¢ç´¢ã§æœ‰æœ›é ˜åŸŸã‚’ç‰¹å®š
    promising_candidates = []
    phase1_candidates = [
        (1, 4, 0), (1, 8, 1), (2, 8, 0), (2, 12, 2), 
        (3, 16, 1), (4, 20, 3), (5, 24, 4)
    ]
    
    for L, B, P in phase1_candidates:
        if B not in BEAM_RANGE or evaluations >= max_evaluations // 2:
            continue
        score = evaluate_with_timeout(inp, L, B, P, timeout=20)
        evaluations += 1
        
        if score < best_score:
            best_score = score
            best_params = (L, B, P)
        
        # ä¸Šä½å€™è£œã¨ã—ã¦è¨˜éŒ²
        promising_candidates.append((L, B, P, score))
    
    # ä¸Šä½å€™è£œã‚’ä¸­å¿ƒã«ç²¾å¯†æ¢ç´¢
    promising_candidates.sort(key=lambda x: x[3])
    top_candidates = promising_candidates[:3]
    
    # ãƒ•ã‚§ãƒ¼ã‚º2: ä¸Šä½å€™è£œå‘¨è¾ºã®ç²¾å¯†æ¢ç´¢
    for base_L, base_B, base_P, _ in top_candidates:
        for dL in [-1, 0, 1]:
            for dB in [-4, 0, 4]:
                for dP in [-1, 0, 1]:
                    if evaluations >= max_evaluations:
                        break
                    
                    L = max(1, min(5, base_L + dL))
                    B = max(4, min(24, base_B + dB))
                    P = max(0, min(4, base_P + dP))
                    
                    if B not in BEAM_RANGE:
                        continue
                    
                    # æ—¢ã«è©•ä¾¡æ¸ˆã¿ã‚’ã‚¹ã‚­ãƒƒãƒ—
                    if (L, B, P) in [(c[0], c[1], c[2]) for c in promising_candidates]:
                        continue
                    
                    score = evaluate_with_timeout(inp, L, B, P, timeout=25)
                    evaluations += 1
                    
                    if score < best_score:
                        best_score = score
                        best_params = (L, B, P)
    
    result = (*best_params, best_score)
    
    # çµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥
    if cache_file:
        try:
            with open(cache_file, 'wb') as f:
                pickle.dump(result, f)
        except:
            pass
    
    return result

# find_best_paramsã‚’ç½®ãæ›ãˆ
find_best_params = intelligent_param_search

# ------------------ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ ------------------
def create_smart_dataset():
    """ã‚¹ãƒãƒ¼ãƒˆã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼†åŠ¹ç‡çš„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ"""
    print("ğŸ“Š ã‚¹ãƒãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆä¸­...")
    
    # å®Ÿãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    if N_REAL_DATA == 0:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {INPUT_DIR} ã«.txtãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return None, None, None
    
    # ã‚¹ãƒãƒ¼ãƒˆã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    if USE_SMART_SAMPLING and N_REAL_DATA > SMART_SAMPLE_SIZE:
        selected_files = analyze_data_diversity(real_data_files)
        print(f"âœ… {len(selected_files)}å€‹ã®ä»£è¡¨ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’")
    else:
        selected_files = real_data_files
        print(f"âœ… å…¨{len(selected_files)}å€‹ã®ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’")
    
    def worker(file_path):
        """1ã¤ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«å¯¾ã—ã¦ãƒ‡ãƒ¼ã‚¿ä½œæˆ"""
        try:
            with open(file_path, 'r') as f:
                inp = f.read().strip()
            
            features = extract_feature(inp)
            L, B, P, score = find_best_params(inp)
            return features, (L, B, P), score, os.path.basename(file_path)
        except Exception as e:
            print(f"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«{file_path}ã§ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    # ä¸¦åˆ—å‡¦ç†ã§é«˜é€ŸåŒ–
    with mp.Pool(MAX_WORKERS) as pool:
        results = list(tqdm(
            pool.imap(worker, selected_files), 
            total=len(selected_files),
            desc="ãƒ‡ãƒ¼ã‚¿å‡¦ç†"
        ))
    
    # Noneé™¤å»
    valid_results = [r for r in results if r is not None]
    print(f"âœ… æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿: {len(valid_results)}/{len(selected_files)}")
    
    if len(valid_results) < len(selected_files) * 0.7:
        print("âš ï¸ æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã™ãã¾ã™ã€‚solver_evalã®å®Ÿè£…ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return None, None, None
    
    X = np.vstack([r[0] for r in valid_results])
    params = np.array([r[1] for r in valid_results])
    scores = np.array([r[2] for r in valid_results])
    filenames = [r[3] for r in valid_results]
    
    print(f"ğŸ“ˆ ã‚¹ã‚³ã‚¢çµ±è¨ˆ: å¹³å‡={np.mean(scores):.1f}, æœ€å°={np.min(scores):.1f}, æœ€å¤§={np.max(scores):.1f}")
    
    # ãƒ‡ãƒãƒƒã‚°æƒ…å ±
    print(f"ğŸ“Š ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åˆ†å¸ƒ:")
    print(f"   L (å…ˆèª­ã¿): å¹³å‡={np.mean(params[:, 0]):.2f}, ç¯„å›²=[{np.min(params[:, 0])}-{np.max(params[:, 0])}]")
    print(f"   B (ãƒ“ãƒ¼ãƒ ): å¹³å‡={np.mean(params[:, 1]):.2f}, ç¯„å›²=[{np.min(params[:, 1])}-{np.max(params[:, 1])}]")
    print(f"   P (ãƒ‘ãƒ¬ãƒƒãƒˆ): å¹³å‡={np.mean(params[:, 2]):.2f}, ç¯„å›²=[{np.min(params[:, 2])}-{np.max(params[:, 2])}]")
    
    return X, params, scores

# create_datasetã‚’ç½®ãæ›ãˆ
create_dataset = create_smart_dataset

# ------------------ ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ ------------------
def train_ensemble_models(X, Y):
    """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ã§ã‚ˆã‚Šé«˜ç²¾åº¦ãªãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ"""
    print("ğŸ¤– ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«è¨“ç·´ä¸­...")
    
    # è¨“ç·´ãƒ»æ¤œè¨¼åˆ†å‰²
    X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=42)
    
    # LightGBMãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆé«˜ç²¾åº¦è¨­å®šï¼‰
    lgb_params = {
        'objective': 'regression',
        'metric': 'rmse',
        'boosting_type': 'gbdt',
        'num_leaves': 63,
        'learning_rate': 0.05,
        'feature_fraction': 0.8,
        'bagging_fraction': 0.8,
        'bagging_freq': 5,
        'verbose': -1,
        'random_state': 42,
        'reg_alpha': 0.1,
        'reg_lambda': 0.1
    }
    
    models = []
    param_names = ['L (lookahead)', 'B (beam)', 'P (palette)']
    
    for i, name in enumerate(param_names):
        print(f"  ğŸ“š {name} ãƒ¢ãƒ‡ãƒ«è¨“ç·´ä¸­...")
        
        if USE_ENSEMBLE:
            # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«: 3ã¤ã®ç•°ãªã‚‹è¨­å®šã§ãƒ¢ãƒ‡ãƒ«ä½œæˆ
            ensemble = []
            for seed in [42, 123, 456]:
                model = lgb.LGBMRegressor(
                    **lgb_params,
                    n_estimators=300,
                    random_state=seed
                )
                model.fit(X_train, Y_train[:, i])
                ensemble.append(model)
            
            # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬ã‚’ãƒ†ã‚¹ãƒˆ
            pred_ensemble = np.mean([m.predict(X_val) for m in ensemble], axis=0)
            mae = np.mean(np.abs(pred_ensemble - Y_val[:, i]))
            
            models.append(ensemble)
        else:
            model = lgb.LGBMRegressor(
                **lgb_params,
                n_estimators=300
            )
            model.fit(X_train, Y_train[:, i])
            
            pred = model.predict(X_val)
            mae = np.mean(np.abs(pred - Y_val[:, i]))
            
            models.append([model])
        
        print(f"    âœ… MAE: {mae:.3f}")
    
    return models

# train_modelsã‚’ç½®ãæ›ãˆ
train_models = train_ensemble_models if USE_ENSEMBLE else train_models

def save_ensemble_models(models):
    """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ã‚’JSONã§ä¿å­˜"""
    print("ğŸ’¾ ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ä¿å­˜ä¸­...")
    
    model_data = []
    for model_ensemble in models:
        if isinstance(model_ensemble, list):
            # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã®å ´åˆã¯å¹³å‡ç”¨ã®ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’ä¿å­˜
            ensemble_data = []
            for model in model_ensemble:
                model_json = model.booster_.dump_model()
                ensemble_data.append(model_json)
            model_data.append(ensemble_data)
        else:
            # å˜ä¸€ãƒ¢ãƒ‡ãƒ«ã®å ´åˆ
            model_json = model_ensemble.booster_.dump_model()
            model_data.append([model_json])
    
    with open(MODEL_JSON, 'w') as f:
        json.dump(model_data, f, indent=2)
    
    print(f"âœ… ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†: {MODEL_JSON}")

# save_modelsã‚’ç½®ãæ›ãˆ
save_models = save_ensemble_models if USE_ENSEMBLE else save_models

# ------------------ ãƒ¡ã‚¤ãƒ³å‡¦ç† ------------------
def main():
    print("ğŸš€ AtCoder AHC048 å¤§é‡ãƒ‡ãƒ¼ã‚¿å¯¾å¿œã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆæœ€é©åŒ–å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ")
    print("=" * 80)
    
    # å®Ÿãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ç¢ºèª
    if not os.path.exists(INPUT_DIR):
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {INPUT_DIR} ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return
    
    if N_REAL_DATA == 0:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {INPUT_DIR} ã«.txtãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return
    
    # solver_evalã®å­˜åœ¨ç¢ºèª
    if not os.path.exists(SOLVER_BIN):
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {SOLVER_BIN} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        print("å…ˆã«solver_eval.cppã‚’ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã—ã¦ãã ã•ã„:")
        print(f"  g++ -O2 -std=c++20 -o {SOLVER_BIN} solver_eval.cpp")
        return
    
    start_time = time.time()
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
    X, params, scores = create_dataset()
    if X is None:
        return
    
    print(f"ğŸ“Š ç‰¹å¾´é‡æ¬¡å…ƒ: {X.shape[1]}")
    print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿æ•°: {X.shape[0]}")
    print(f"ğŸ“Š å®Ÿãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹: {N_REAL_DATA}ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆã‚¹ãƒãƒ¼ãƒˆã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°é©ç”¨ï¼‰")
    
    # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
    models = train_models(X, params)
    
    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
    save_models(models)
    
    # å­¦ç¿’æ™‚é–“
    elapsed = time.time() - start_time
    print(f"â±ï¸ å­¦ç¿’å®Œäº†: {elapsed:.1f}ç§’")
    print(f"ğŸ‰ é«˜ç²¾åº¦{MODEL_JSON}ãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸï¼")
    print(f"ğŸ’ 500å€‹ã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å­¦ç¿’ã—ãŸæœ€å¼·ãƒ¢ãƒ‡ãƒ«ã‚ˆï¼")
    print("\næ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
    print("1. python gen_header.py ã§C++ãƒ˜ãƒƒãƒ€ç”Ÿæˆ")
    print("2. solver.cppã‚’ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã—ã¦æœ¬ç•ªå®Ÿè¡Œ")

if __name__ == "__main__":
    main() 